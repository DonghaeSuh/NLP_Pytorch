{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jLd3fgb_ouffcYMR_qbngEdXeBW7cdyx",
      "authorship_tag": "ABX9TyPd/DwSgNgqG6XLcLUwPFqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/NLP_Pytorch/blob/main/Model/GPT_2/GPT2_GEN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentencepiece : https://github.com/google/sentencepiece/blob/master/python/README.md"
      ],
      "metadata": {
        "id": "e33Dl5K-J_8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NSMC"
      ],
      "metadata": {
        "id": "GC2mkEpqPcB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib"
      ],
      "metadata": {
        "id": "GAhQpLHSKpwQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt','train.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGWRVyXKZjN",
        "outputId": "d7a99aa7-08a3-4f86-caaa-e0132862dfd5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('train.txt', <http.client.HTTPMessage at 0x79a87b7f9b40>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE7jFu1dpEiX",
        "outputId": "d9d948ef-5ba4-4b34-ae67-9262190808cf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp=[]"
      ],
      "metadata": {
        "id": "_D-LYLxqLhUP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.txt','r') as f:\n",
        "  f.readline()\n",
        "  for sent in f.readlines():\n",
        "    data = sent.split('\\t')\n",
        "    temp.append(data[1])"
      ],
      "metadata": {
        "id": "KRfZtCCTK8ht"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Crf3PXHL7x8",
        "outputId": "afff9864-c9cd-417c-a56b-5f95920e3197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Pytorch\\ NLP/GPT-2/data_in/gpt2_ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG3ehNgKKuDL",
        "outputId": "eccb70fe-aee4-4f40-b322-6515874ad5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in/gpt2_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('corpus.txt','w') as f:\n",
        "  for sent in temp:\n",
        "    f.write(f'\\n {sent}')"
      ],
      "metadata": {
        "id": "NaCJp9IsLLiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentencepiece"
      ],
      "metadata": {
        "id": "B2LJdAsrPbf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keep-steady.tistory.com/7"
      ],
      "metadata": {
        "id": "mBkBxNqCSM2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APOTeyaOOQYb",
        "outputId": "2fbbedc1-ebf5-4891-ed6f-5c66484f215b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.0/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "ZayDEQQtPgz3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(input='corpus.txt',model_prefix='kor_tokenizer',vocab_size=50257, model_type='bpe',max_sentence_length=9999999,pad_id=0,unk_id=1,bos_id=2,eos_id=3)"
      ],
      "metadata": {
        "id": "Z8G5cAI3PpbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "bCjotYQ6U329"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp=spm.SentencePieceProcessor()\n",
        "sp.Load('kor_tokenizer.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quvfnYo9rqdD",
        "outputId": "69d947e5-e147-44e1-d00f-fb2c49503e87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.EncodeAsPieces('안녕하세요 저는 사람이에요.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdcRqlOLrYr-",
        "outputId": "ca9a4b9f-2dce-467a-82c0-bdcfff4499db"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁안녕하세요', '▁저는', '▁사람이', '에요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode('안녕하세요 저는 사람이에요.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9ZNdSMwsE1T",
        "outputId": "8b9e3e60-badb-494e-f3bc-1c8f47f8dfdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20363, 1619, 1047, 802, 48546]"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode('그래')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eT_jqTlkfT0",
        "outputId": "8c0791eb-d787-4c11-aea0-257506f80bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[312]"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.bos_id()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvfou9ZbktX3",
        "outputId": "b70565ed-c8c3-49ac-c14c-30f1abdfb1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]"
      ],
      "metadata": {
        "id": "G4ZIARzH8IM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('0 :{}, 1 :{} , 2 :{}, 3 :{} '.format(vocab[0],vocab[1],vocab[2],vocab[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf1TMOXiioo2",
        "outputId": "5f85aa86-6ce1-4a53-c4db-37a758a2b2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :<pad>, 1 :<unk> , 2 :<s>, 3 :</s> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input for fine-tuning"
      ],
      "metadata": {
        "id": "EnUMRMLQwt3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtering"
      ],
      "metadata": {
        "id": "VxowShiSyv5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Q5B8v-HhxFbE"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CMQA27SOx1jR",
        "outputId": "4b163680-8fa2-462c-90a3-8138f9b6b823"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sent = [ ' '.join((re.sub('[^가-힣0-9 ]',' ',sent)).split()) for sent in temp]"
      ],
      "metadata": {
        "id": "Wop314V0w367"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sent[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GTlyEk2Hxnv-",
        "outputId": "10e7ba67-7193-4180-ddf3-ab68ce7021a1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'볼때마다 눈물나서 죽겠다90년대의 향수자극 허진호는 감성절제멜로의 달인이다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### padding and truncation"
      ],
      "metadata": {
        "id": "OoKIFQIhy7wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "t9pzX7r4y6_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_len=[len(sp.encode(sent)) for sent in filtered_sent ]"
      ],
      "metadata": {
        "id": "XWNRSipb0CwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('3사분위 길이 :{}'.format(np.percentile(tokenized_len,99)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EYC0ibBzC_W",
        "outputId": "40fedf52-7f04-4e32-84aa-f40ca1ac2182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3사분위 길이 :47.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN=47"
      ],
      "metadata": {
        "id": "viy2W_q5mM0k"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ka_qkUUAp7Ei"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_output(input,max_len):\n",
        "  # input : (samples, sent_len)\n",
        "\n",
        "  train_input=[]\n",
        "  train_output=[]\n",
        "\n",
        "  # append <s>, </s>, <pad>\n",
        "  for sent in input:\n",
        "    if len(sp.encode(sent))<max_len:\n",
        "      pad_len = max_len-len(sp.encode(sent))-1\n",
        "      train_input.append([sp.bos_id()]+sp.encode(sent)+[sp.pad_id()]*pad_len)\n",
        "      train_output.append(sp.encode(sent)+[sp.eos_id()]+[sp.pad_id()]*pad_len)\n",
        "    else: # truncation\n",
        "      train_input.append([sp.bos_id()]+sp.encode(sent)[:max_len-1])\n",
        "      train_output.append(sp.encode(sent)[:max_len-1]+[sp.eos_id()])\n",
        "\n",
        "  return torch.LongTensor(train_input), torch.LongTensor(train_output) # (samples, max_len)"
      ],
      "metadata": {
        "id": "vd_P_Hp3mSYx"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_output = preprocess_input_output(filtered_sent,MAX_LEN)"
      ],
      "metadata": {
        "id": "mPjclEZApuzF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6pNb_feqHEO",
        "outputId": "4a8d27d0-3133-4cbb-daae-9c7ab1a52e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    2,     8,  1080,    55, 17382,  2106,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow_AgLq0qa63",
        "outputId": "50df4070-ac03-49e3-df93-2ae8b681e31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    8,  1080,    55, 17382,  2106,     3,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader"
      ],
      "metadata": {
        "id": "UrALBW-GqeDu"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.size()[0]"
      ],
      "metadata": {
        "id": "eNa1vjm2qjIl"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_input,train_output)"
      ],
      "metadata": {
        "id": "DIP9A89F4G9H"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = DataLoader(train_dataset,batch_size=64)"
      ],
      "metadata": {
        "id": "bBvvF-W65VcD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uujliuwn9D2w",
        "outputId": "4fbb9e6a-4f93-45ec-8641-88c17e4d30c4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[    2,     8,  1080,  ...,     0,     0,     0],\n",
              "         [    2,  1609, 17387,  ...,     0,     0,     0],\n",
              "         [    2,    25, 48579,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [    2,   194, 48700,  ...,     0,     0,     0],\n",
              "         [    2, 39386,  1661,  ...,     0,     0,     0],\n",
              "         [    2,  4928,  3547,  ...,     0,     0,     0]]),\n",
              " tensor([[    8,  1080,    55,  ...,     0,     0,     0],\n",
              "         [ 1609, 17387, 23046,  ...,     0,     0,     0],\n",
              "         [   25, 48579, 50166,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  194, 48700, 48547,  ...,     0,     0,     0],\n",
              "         [39386,  1661, 38441,  ...,     0,     0,     0],\n",
              "         [ 4928,  3547,   610,  ...,     0,     0,     0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### config & model_weight"
      ],
      "metadata": {
        "id": "SgiJQ2gTQn-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://huggingface.co/gpt2/resolve/main/config.json','config.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeqjKoUcQs2X",
        "outputId": "f4630f7e-722f-490c-98ca-fb15eaf06473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('config.json', <http.client.HTTPMessage at 0x7fdd07c1ada0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://huggingface.co/gpt2/resolve/main/model.safetensors','model.safetensors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXbpfCuvRWr2",
        "outputId": "fe5048c1-24e1-4700-cc03-420c0ad62500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model.safetensors', <http.client.HTTPMessage at 0x7fdd07c19ed0>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Pytorch\\ NLP/GPT-2/data_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkJ_wyufzQc",
        "outputId": "45892256-6e46-4283-de6c-fc8a7a82a6b5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic5DesXsf_zW",
        "outputId": "f52fe719-ff77-4046-9084-879a3df381f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=4ab74ff4e824669769cb28e4fd058e4a74225cd1b74e2f8689bd7d1bba7de6a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import zipfile\n",
        "\n",
        "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
        "\n",
        "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
        "    z.extractall()"
      ],
      "metadata": {
        "id": "Y9IFr_LHfxtU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top k & Top p"
      ],
      "metadata": {
        "id": "0kjeBZ0bOhhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.multinomial"
      ],
      "metadata": {
        "id": "tuhXFcHsBYNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "SP56xQAnBlXK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "  # logits = (vocab_size, )\n",
        "\n",
        "  if top_k>0:\n",
        "    top_k=min(top_k,logits.size()[0])\n",
        "    indices_to_remove=logits<torch.topk(logits,top_k).values[-1] # smaller than the smallest value among the top_k values\n",
        "    logits[indices_to_remove]=filter_value\n",
        "\n",
        "  if top_p>0.0:\n",
        "    sorted_logits=torch.sort(logits,descending=True)\n",
        "    sorted_logits_index=torch.argsort(logits,descending=True)\n",
        "    prob_cumsum = torch.cumsum(nn.Softmax()(sorted_logits))\n",
        "\n",
        "    sorted_indices_to_remove=prob_cumsum>top_p\n",
        "    sorted_indices_to_remove=torch.cat([torch.LongTensor([False]),sorted_indices_to_remove[:-1]]) # prevent if first is True\n",
        "    indices_to_remove = sorted_logits_index[sorted_indices_to_remove]\n",
        "\n",
        "    logits[indices_to_remove]=filter_value\n",
        "\n",
        "  return logits   # logits = (vocab_size, )\n"
      ],
      "metadata": {
        "id": "Y3gxMcb8OxxL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy & Generate_sentence Function"
      ],
      "metadata": {
        "id": "-c0qYyVEqaoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sent(seed_word,model,max_len=100,greedy=False,top_k=0,top_p=0.0):\n",
        "  sent= seed_word\n",
        "  toked = sp.encode(sent)\n",
        "\n",
        "  for _ in range(max_len):\n",
        "    input_ids = torch.LongTensor([sp.bos_id()]+toked)[None,:] # input_ids = (1, cumulated_seq_len)\n",
        "    outputs = model(input_ids)[0,-1,:] # outputs : (vocab_size, )\n",
        "\n",
        "    if greedy:\n",
        "      gen = sp.id_to_piece(outputs.argmax().tolist()) # outputs.argmax().tolist() -> int\n",
        "    else:\n",
        "      output_logits = top_k_top_p_filtering(outputs,top_k,top_p) # logits = (vocab_size, )\n",
        "      gen = torch.multinomial(nn.Softmax(-1)(output_logits),num_samples=1,replacement=True).tolist()[0]\n",
        "      gen = sp.id_to_piece(gen)\n",
        "    if gen == '</s>':\n",
        "      break\n",
        "\n",
        "    sent+=gen.replace('▁',' ')\n",
        "    toked=sp.encode(sent)\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "7FD4jcD3eLBS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface Transformers"
      ],
      "metadata": {
        "id": "2tF56U0HqZSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au-SM5_crBt4",
        "outputId": "04a83755-1425-468f-fe05-d9d5a0564127"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "uG_2d-qWo3Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1a2c28-0fe2-4a91-c62a-ffb65d2f8c6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "oVRNhprurFxi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Pre-training"
      ],
      "metadata": {
        "id": "GPOoiqIA7Fd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Gen(nn.Module):\n",
        "  def __init__(self,dir_path):\n",
        "    super(GPT2Gen,self).__init__()\n",
        "    self.gpt2 = GPT2LMHeadModel.from_pretrained(dir_path,ignore_mismatched_sizes=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    outputs=self.gpt2(x)[0]\n",
        "    return torch.transpose(outputs,2,1) # (batch_size=1, vocab_size, max_len)"
      ],
      "metadata": {
        "id": "nz33hkvr74Tv"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter\n",
        "MAX_LEN=47\n",
        "BATCH_SIZE=32\n",
        "EPOCHS=10\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "093hz24Q8fN3"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Gen('./data_in/gpt2_ckpt').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=3e-5)"
      ],
      "metadata": {
        "id": "bcTT4Y-U7-nG"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in model.named_parameters():\n",
        "  if not name=='gpt2.transformer.wte.weight':\n",
        "    param.requires_grad=False"
      ],
      "metadata": {
        "id": "4oZlRsMhcYOO"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in model.named_parameters():\n",
        "  print(f'{name}, {param.requires_grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4IjYxsc-5h",
        "outputId": "fdf7b2d6-4d20-4388-951a-dc5ba12f1dd8"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt2.transformer.wte.weight, True\n",
            "gpt2.transformer.wpe.weight, False\n",
            "gpt2.transformer.h.0.ln_1.weight, False\n",
            "gpt2.transformer.h.0.ln_1.bias, False\n",
            "gpt2.transformer.h.0.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.0.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.0.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.0.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.0.ln_2.weight, False\n",
            "gpt2.transformer.h.0.ln_2.bias, False\n",
            "gpt2.transformer.h.0.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.0.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.0.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.0.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.1.ln_1.weight, False\n",
            "gpt2.transformer.h.1.ln_1.bias, False\n",
            "gpt2.transformer.h.1.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.1.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.1.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.1.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.1.ln_2.weight, False\n",
            "gpt2.transformer.h.1.ln_2.bias, False\n",
            "gpt2.transformer.h.1.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.1.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.1.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.1.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.2.ln_1.weight, False\n",
            "gpt2.transformer.h.2.ln_1.bias, False\n",
            "gpt2.transformer.h.2.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.2.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.2.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.2.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.2.ln_2.weight, False\n",
            "gpt2.transformer.h.2.ln_2.bias, False\n",
            "gpt2.transformer.h.2.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.2.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.2.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.2.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.3.ln_1.weight, False\n",
            "gpt2.transformer.h.3.ln_1.bias, False\n",
            "gpt2.transformer.h.3.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.3.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.3.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.3.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.3.ln_2.weight, False\n",
            "gpt2.transformer.h.3.ln_2.bias, False\n",
            "gpt2.transformer.h.3.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.3.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.3.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.3.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.4.ln_1.weight, False\n",
            "gpt2.transformer.h.4.ln_1.bias, False\n",
            "gpt2.transformer.h.4.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.4.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.4.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.4.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.4.ln_2.weight, False\n",
            "gpt2.transformer.h.4.ln_2.bias, False\n",
            "gpt2.transformer.h.4.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.4.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.4.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.4.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.5.ln_1.weight, False\n",
            "gpt2.transformer.h.5.ln_1.bias, False\n",
            "gpt2.transformer.h.5.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.5.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.5.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.5.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.5.ln_2.weight, False\n",
            "gpt2.transformer.h.5.ln_2.bias, False\n",
            "gpt2.transformer.h.5.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.5.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.5.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.5.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.6.ln_1.weight, False\n",
            "gpt2.transformer.h.6.ln_1.bias, False\n",
            "gpt2.transformer.h.6.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.6.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.6.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.6.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.6.ln_2.weight, False\n",
            "gpt2.transformer.h.6.ln_2.bias, False\n",
            "gpt2.transformer.h.6.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.6.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.6.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.6.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.7.ln_1.weight, False\n",
            "gpt2.transformer.h.7.ln_1.bias, False\n",
            "gpt2.transformer.h.7.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.7.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.7.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.7.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.7.ln_2.weight, False\n",
            "gpt2.transformer.h.7.ln_2.bias, False\n",
            "gpt2.transformer.h.7.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.7.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.7.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.7.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.8.ln_1.weight, False\n",
            "gpt2.transformer.h.8.ln_1.bias, False\n",
            "gpt2.transformer.h.8.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.8.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.8.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.8.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.8.ln_2.weight, False\n",
            "gpt2.transformer.h.8.ln_2.bias, False\n",
            "gpt2.transformer.h.8.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.8.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.8.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.8.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.9.ln_1.weight, False\n",
            "gpt2.transformer.h.9.ln_1.bias, False\n",
            "gpt2.transformer.h.9.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.9.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.9.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.9.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.9.ln_2.weight, False\n",
            "gpt2.transformer.h.9.ln_2.bias, False\n",
            "gpt2.transformer.h.9.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.9.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.9.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.9.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.10.ln_1.weight, False\n",
            "gpt2.transformer.h.10.ln_1.bias, False\n",
            "gpt2.transformer.h.10.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.10.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.10.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.10.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.10.ln_2.weight, False\n",
            "gpt2.transformer.h.10.ln_2.bias, False\n",
            "gpt2.transformer.h.10.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.10.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.10.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.10.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.11.ln_1.weight, False\n",
            "gpt2.transformer.h.11.ln_1.bias, False\n",
            "gpt2.transformer.h.11.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.11.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.11.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.11.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.11.ln_2.weight, False\n",
            "gpt2.transformer.h.11.ln_2.bias, False\n",
            "gpt2.transformer.h.11.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.11.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.11.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.11.mlp.c_proj.bias, False\n",
            "gpt2.transformer.ln_f.weight, False\n",
            "gpt2.transformer.ln_f.bias, False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8dwuo8L69JBP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.Tensor([[1,1,1,0,0],[1,1,0,0,0]])\n",
        "mask=~(a==0)\n",
        "\n",
        "b=torch.Tensor([[3,2,5,0,0],[1,5,3,1,0]])\n",
        "(b*mask).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucore8EoA3D_",
        "outputId": "1fa6dbd5-f68d-4810-aebd-76029fe49bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(outputs,y,criterion):\n",
        "  loss = criterion(outputs,y) # (batch_size, max_len)\n",
        "  mask = ~(y==0)\n",
        "\n",
        "  return (loss*mask).sum()"
      ],
      "metadata": {
        "id": "SwvBnFth_7Ml"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer,criterion,train_iter):\n",
        "  model.train()\n",
        "  losses=[]\n",
        "\n",
        "  total_loss = 0\n",
        "  for batch in tqdm(train_iter):\n",
        "    x=batch[0].to(device)\n",
        "    y=batch[1].to(device)\n",
        "\n",
        "    outputs = model(x) # outputs : (batch_size, vocab_size, max_len)\n",
        "    loss = compute_loss(outputs,y,criterion) # (batch_size, max_len )\n",
        "    total_loss+=loss\n",
        "    losses.append(loss)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return total_loss/len(train_iter), losses"
      ],
      "metadata": {
        "id": "U3W9CSsX75MN"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "88Ea4Y_vEPqw"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = None\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  avg_loss, losses=train(model,optimizer,criterion,train_iter)\n",
        "  print('avg_loss : {}'.format(avg_loss))\n",
        "\n",
        "  if not best_loss or avg_loss<best_loss:\n",
        "    os.makedirs('./data_out/best_model')\n",
        "    torch.save(model.state_dict(),'./data_out/best_model/best_weight.pt')\n",
        "    best_loss=avg_loss"
      ],
      "metadata": {
        "id": "zfnyJcctCc-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "8efa8b1c-d5fd-4019-a2a1-06c2c7bfe615"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2344 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-55bb716d4f53>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg_loss : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-ab62d2f15332>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_iter)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# outputs : (batch_size, vocab_size, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-156853e8e38f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size=1, vocab_size, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m                 )\n\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    901\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 14.75 GiB total capacity; 13.67 GiB already allocated; 20.81 MiB free; 13.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses"
      ],
      "metadata": {
        "id": "dlcIqAidqAs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVeuPfFNM2Ta",
        "outputId": "a07f4985-8dd7-45c0-c534-0d744ea07e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['gpt2.transformer.wte.weight', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.h.6.ln_1.weight', 'gpt2.transformer.h.6.ln_1.bias', 'gpt2.transformer.h.6.attn.c_attn.weight', 'gpt2.transformer.h.6.attn.c_attn.bias', 'gpt2.transformer.h.6.attn.c_proj.weight', 'gpt2.transformer.h.6.attn.c_proj.bias', 'gpt2.transformer.h.6.ln_2.weight', 'gpt2.transformer.h.6.ln_2.bias', 'gpt2.transformer.h.6.mlp.c_fc.weight', 'gpt2.transformer.h.6.mlp.c_fc.bias', 'gpt2.transformer.h.6.mlp.c_proj.weight', 'gpt2.transformer.h.6.mlp.c_proj.bias', 'gpt2.transformer.h.7.ln_1.weight', 'gpt2.transformer.h.7.ln_1.bias', 'gpt2.transformer.h.7.attn.c_attn.weight', 'gpt2.transformer.h.7.attn.c_attn.bias', 'gpt2.transformer.h.7.attn.c_proj.weight', 'gpt2.transformer.h.7.attn.c_proj.bias', 'gpt2.transformer.h.7.ln_2.weight', 'gpt2.transformer.h.7.ln_2.bias', 'gpt2.transformer.h.7.mlp.c_fc.weight', 'gpt2.transformer.h.7.mlp.c_fc.bias', 'gpt2.transformer.h.7.mlp.c_proj.weight', 'gpt2.transformer.h.7.mlp.c_proj.bias', 'gpt2.transformer.h.8.ln_1.weight', 'gpt2.transformer.h.8.ln_1.bias', 'gpt2.transformer.h.8.attn.c_attn.weight', 'gpt2.transformer.h.8.attn.c_attn.bias', 'gpt2.transformer.h.8.attn.c_proj.weight', 'gpt2.transformer.h.8.attn.c_proj.bias', 'gpt2.transformer.h.8.ln_2.weight', 'gpt2.transformer.h.8.ln_2.bias', 'gpt2.transformer.h.8.mlp.c_fc.weight', 'gpt2.transformer.h.8.mlp.c_fc.bias', 'gpt2.transformer.h.8.mlp.c_proj.weight', 'gpt2.transformer.h.8.mlp.c_proj.bias', 'gpt2.transformer.h.9.ln_1.weight', 'gpt2.transformer.h.9.ln_1.bias', 'gpt2.transformer.h.9.attn.c_attn.weight', 'gpt2.transformer.h.9.attn.c_attn.bias', 'gpt2.transformer.h.9.attn.c_proj.weight', 'gpt2.transformer.h.9.attn.c_proj.bias', 'gpt2.transformer.h.9.ln_2.weight', 'gpt2.transformer.h.9.ln_2.bias', 'gpt2.transformer.h.9.mlp.c_fc.weight', 'gpt2.transformer.h.9.mlp.c_fc.bias', 'gpt2.transformer.h.9.mlp.c_proj.weight', 'gpt2.transformer.h.9.mlp.c_proj.bias', 'gpt2.transformer.h.10.ln_1.weight', 'gpt2.transformer.h.10.ln_1.bias', 'gpt2.transformer.h.10.attn.c_attn.weight', 'gpt2.transformer.h.10.attn.c_attn.bias', 'gpt2.transformer.h.10.attn.c_proj.weight', 'gpt2.transformer.h.10.attn.c_proj.bias', 'gpt2.transformer.h.10.ln_2.weight', 'gpt2.transformer.h.10.ln_2.bias', 'gpt2.transformer.h.10.mlp.c_fc.weight', 'gpt2.transformer.h.10.mlp.c_fc.bias', 'gpt2.transformer.h.10.mlp.c_proj.weight', 'gpt2.transformer.h.10.mlp.c_proj.bias', 'gpt2.transformer.h.11.ln_1.weight', 'gpt2.transformer.h.11.ln_1.bias', 'gpt2.transformer.h.11.attn.c_attn.weight', 'gpt2.transformer.h.11.attn.c_attn.bias', 'gpt2.transformer.h.11.attn.c_proj.weight', 'gpt2.transformer.h.11.attn.c_proj.bias', 'gpt2.transformer.h.11.ln_2.weight', 'gpt2.transformer.h.11.ln_2.bias', 'gpt2.transformer.h.11.mlp.c_fc.weight', 'gpt2.transformer.h.11.mlp.c_fc.bias', 'gpt2.transformer.h.11.mlp.c_proj.weight', 'gpt2.transformer.h.11.mlp.c_proj.bias', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.ln_f.bias', 'gpt2.lm_head.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGwm5vhITZN-",
        "outputId": "593107ff-a40e-423f-dfd2-b41a2253d11c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Gen(\n",
              "  (gpt2): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generative Model"
      ],
      "metadata": {
        "id": "4JZU6PS-rdS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK5seQ5Qr8eI",
        "outputId": "9d961767-e3c4-4b74-fac0-c83a3cd0c832"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Gen(nn.Module):\n",
        "  def __init__(self,dir_path):\n",
        "    super(GPT2Gen,self).__init__()\n",
        "    self.gpt2 = GPT2LMHeadModel.from_pretrained(dir_path,ignore_mismatched_sizes=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.gpt2(x)[0] # (batch_size=1, cumulated_seq_len, vocab_size =500257)"
      ],
      "metadata": {
        "id": "O-FSehzxrOCz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model = GPT2Gen('./data_in/gpt2_ckpt')"
      ],
      "metadata": {
        "id": "5H6yvm_Jsdw3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate_sentences"
      ],
      "metadata": {
        "id": "slaZ1EEntEWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('data_out/best_model/best_weight.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp5ytgvIlqnr",
        "outputId": "f58fe500-7354-4318-e9c2-4f214411b6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfCwuzOsmrD0",
        "outputId": "3fefd158-76c3-4a65-a657-55769e782fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Gen(\n",
              "  (gpt2): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent('안녕',gpt2_model,max_len=10,top_k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TK_KLic3tB6P",
        "outputId": "e8e8b50b-c1cb-4c90-ad12-a4fae0b86dae"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕 차 그래도 보겠다 이수 그근한 포스터만드라 그 커'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "mxImBf-LtQwz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}