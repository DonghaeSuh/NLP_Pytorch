{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jLd3fgb_ouffcYMR_qbngEdXeBW7cdyx",
      "authorship_tag": "ABX9TyOdOPKmQJXPvRz1C2vmApjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/NLP_Pytorch/blob/main/Model/GPT_2/GPT2_GEN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentencepiece : https://github.com/google/sentencepiece/blob/master/python/README.md"
      ],
      "metadata": {
        "id": "e33Dl5K-J_8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 신문 말뭉치 2022"
      ],
      "metadata": {
        "id": "Q8-ETZTpolOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Pytorch\\ NLP/GPT-2/data_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd8Bt-9Eorci",
        "outputId": "f9ded6c1-6c6a-4902-b391-aa958dc6e122"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "EO01QP2Xqoyb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.json') as f:\n",
        "  train=json.load(f)"
      ],
      "metadata": {
        "id": "29r4GgPCo0sr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint"
      ],
      "metadata": {
        "id": "ZlKSROufqrST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(train['document'][:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtNCDjIjqwYU",
        "outputId": "e02f60ef-5610-4897-fca2-f422598d5402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'NPRW2200000004.1',\n",
            "  'metadata': {'author': '뉴욕=백종민',\n",
            "               'date': '20210101',\n",
            "               'original_topic': '정치,북한|정치,외교|국제,미국_북미',\n",
            "               'publisher': '아시아경제',\n",
            "               'title': '아시아경제 2021년 기사',\n",
            "               'topic': '사회'},\n",
            "  'paragraph': [{'form': '“북, 유럽의회에 미국과 좋은 관계 희망”', 'id': 'NPRW2200000004.1.1'},\n",
            "                {'form': '북한이 최근 유럽의회와 접촉해 ‘미국과 좋은 관계를 원한다’는 뜻을 밝혔다는 보도가 나왔다. '\n",
            "                         '조 바이든 미국 행정부 출범을 앞둔 상황에서 북한이 도발보다는 대화에 방점을 둔 것인지 '\n",
            "                         '주목된다.',\n",
            "                 'id': 'NPRW2200000004.1.2'},\n",
            "                {'form': '월스트리트저널(WSJ)은 31일(현지시간) 소식통을 인용, 북한이 지난 11월 미 대선이 '\n",
            "                         '열리기 며칠 전 유럽의회 한반도관계대표단과 접촉해 온라인 면담을 요청해 성사됐다고 전했다.',\n",
            "                 'id': 'NPRW2200000004.1.3'},\n",
            "                {'form': '이번 면담은 주베를린 북한대사관이 제안했고 오스트리아 출신의 루카스 만들 유럽의회 '\n",
            "                         '한반도관계대표단 회장이 참석해 12월 초에 열렸다.',\n",
            "                 'id': 'NPRW2200000004.1.4'},\n",
            "                {'form': '한 시간가량의 화상회의에서 북한 대사는 미국이 대북 적대 정책을 중단한다면 미국과 강한 관계를 '\n",
            "                         '구축하는 것이 목표라고 여러 번 언급했다고 소식통들이 전했다.',\n",
            "                 'id': 'NPRW2200000004.1.5'},\n",
            "                {'form': '한 소식통은 북한의 이런 입장이 긍정적으로 받아들여졌다고 WSJ에 말했다. 북한과 유럽의회는 '\n",
            "                         '또 이르면 내년 가을 양측 대표단의 상호 방문에 대한 희망도 언급했다고 한다.',\n",
            "                 'id': 'NPRW2200000004.1.6'},\n",
            "                {'form': '이번 면담과 관련해 만들 회장은 북한과의 대화 사실을 확인했으나, 주베를린 북한대사관 측은 '\n",
            "                         '아무런 답을 하지 않았다고 WSJ은 전했다.',\n",
            "                 'id': 'NPRW2200000004.1.7'}]},\n",
            " {'id': 'NPRW2200000004.2',\n",
            "  'metadata': {'author': '김영은',\n",
            "               'date': '20210101',\n",
            "               'original_topic': 'IT_과학,콘텐츠|IT_과학,인터넷_SNS|',\n",
            "               'publisher': '아시아경제',\n",
            "               'title': '아시아경제 2021년 기사',\n",
            "               'topic': '생활'},\n",
            "  'paragraph': [{'form': '“새해 복 많이 받으세요” 코로나 시국 신년회도 온라인으로',\n",
            "                 'id': 'NPRW2200000004.2.1'},\n",
            "                {'form': '신종 코로나바이러스 감염증 (코로나19) 확산 방지를 위해 전국적으로 ‘5인 이상 집합 금지’ '\n",
            "                         '조치가 시행되는 등 방역이 강화하면서 연말연시를 맞아 신년회 모임도 비대면인 온라인으로 '\n",
            "                         '계획하는 시민들이 늘어나고 있다.',\n",
            "                 'id': 'NPRW2200000004.2.2'},\n",
            "                {'form': '또 사회적 거리두기가 강화되면서 저녁 9시 이후 음식점과 술집의 영업이 불가능해졌고 연말연시 '\n",
            "                         '모임 장소로 인기 있는 ‘파티룸’은 운영 자체가 전면 중단됐다.',\n",
            "                 'id': 'NPRW2200000004.2.3'},\n",
            "                {'form': '이처럼 연말 송년회, 종무식, 신년회 등 한 해를 마무리하기 위한 오프라인 모임이 불가능해지자 '\n",
            "                         '줌, 구글 미트 등과 같은 온라인 플랫폼을 활용해 이른바 ‘랜선 신년회’를 진행하는 기업이나 '\n",
            "                         '단체가 늘고 있다.',\n",
            "                 'id': 'NPRW2200000004.2.4'},\n",
            "                {'form': '취업포털 인크루트가 기업회원 663명을 대상으로 ‘2020 연말 사내 행사 계획’에 대한 '\n",
            "                         '조사를 진행한 결과 송년회·종무식 등 연말 행사를 계획하고 있다고 밝힌 곳은 전체 참여기업의 '\n",
            "                         '9.0%에 불과했다. 이는 작년 동기(66.2%)와 비교했을 때 7분의 1 수준으로 줄어든 '\n",
            "                         '수치다.',\n",
            "                 'id': 'NPRW2200000004.2.5'},\n",
            "                {'form': '지난달 26일 인공지능(AI) 동영상 후기 서비스 ‘브이리뷰’를 운영하는 인덴트코퍼레이션은 '\n",
            "                         '비대면 송년회 ‘인덴터의 밤이에요’를 진행했다. 해당 송년회는 전 직원이 참여한 랜선 파티 '\n",
            "                         '형식으로, 화상 회의 프로그램 ‘줌’을 통해 진행됐으며 개회식부터 직원 참여 이벤트, 시상식, '\n",
            "                         '폐회사까지 실제 오프라인 송년회 식순처럼 기획됐다.',\n",
            "                 'id': 'NPRW2200000004.2.6'},\n",
            "                {'form': '롯데홈쇼핑은 31일 코로나19 여파로 대면 접촉을 최소화하는 근무 환경에 따라 직원들을 '\n",
            "                         '격려하고 결속을 다지는 의미를 담아 유튜브를 통한 비대면 송년회를 진행한다고 밝혔다.',\n",
            "                 'id': 'NPRW2200000004.2.7'},\n",
            "                {'form': '또 온라인 클래스 플랫폼인 ‘클래스101’도 다양한 분야에서 활약한 구성원들에게 상을 전달하는 '\n",
            "                         '온라인 어워즈 등을 포함한 랜선 송년회를 계획했다고 전했으며, 이 밖에도 기업용 모바일 식권 '\n",
            "                         '‘식권대장’을 운영하는 벤디스는 배달의민족 쿠폰으로 ‘랜선 회식비’를 지원하는 등 여러 '\n",
            "                         '기업에서 비대면 연말 행사를 진행했다.',\n",
            "                 'id': 'NPRW2200000004.2.8'},\n",
            "                {'form': '기업에서뿐만 아니라 최근 온라인 커뮤니티 및 사회관계망서비스(SNS)에서도 20·30세대를 '\n",
            "                         '중심으로 연말 모임을 비대면으로 갖는 ‘랜선 송년회’를 했다는 게시글들이 이어졌다.',\n",
            "                 'id': 'NPRW2200000004.2.9'},\n",
            "                {'form': '또한 유튜브 등에서는 ‘홈파티 음식 레시피’, ‘랜선 송년회 게임 추천’, ‘랜선 파티 '\n",
            "                         '브이로그’ 등의 콘텐츠들이 잇달아 올라오고 있으며, 다가오는 2021년 신년 맞이 모임 역시 '\n",
            "                         '비대면으로 계획 중이라는 사람들도 늘어나고 있다.',\n",
            "                 'id': 'NPRW2200000004.2.10'},\n",
            "                {'form': '지난 26일 고등학교 동창들과 ‘랜선 송년회’를 한 대학생 A 씨는 “예쁘게 꾸미고 각자 '\n",
            "                         '음식도 준비해서 줌으로 오랜만에 친구들 얼굴을 봤다”라고 전했다.',\n",
            "                 'id': 'NPRW2200000004.2.11'},\n",
            "                {'form': '이어 “유튜브를 보니까 저희같이 랜선 송년회를 하는 사람들을 위한 게임도 많고 콘텐츠들도 '\n",
            "                         '다양해서 여러모로 많이 참고했다”라면서 “화상이긴 하지만 실제로 만난 것처럼 즐거웠고 3시간 '\n",
            "                         '넘게 이야기가 끊이지 않아서 방에서도 연말 분위기를 낼 수 있었다”라고 말했다.',\n",
            "                 'id': 'NPRW2200000004.2.12'},\n",
            "                {'form': '또 A 씨는 “당장 만날 수 있으면 좋겠지만 상황이 이렇다 보니 너무 답답했는데 노트북 '\n",
            "                         '화면으로라도 친구들을 보니까 조금이나마 힐링 된 기분”이라며 “다음에는 꼭 직접 얼굴을 볼 수 '\n",
            "                         '있게 되었으면 좋겠다”라고 전했다.',\n",
            "                 'id': 'NPRW2200000004.2.13'}]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train['document'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV1PLsMrdYZ",
        "outputId": "ba30d17b-b430-4e8b-8b6d-b0334c60f302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81389"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "jG4oKjftsT-U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=[]\n",
        "\n",
        "for sample in tqdm(train['document']):\n",
        "  train_data+=[para['form'] for para in sample['paragraph']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMPfkm5jsAiV",
        "outputId": "27723448-5e7f-40cd-a728-de530f883f11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81389/81389 [00:00<00:00, 565570.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-731H3WouZ4Y",
        "outputId": "64313a4e-edc7-40e6-e9c1-b10c3c421e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“북, 유럽의회에 미국과 좋은 관계 희망”',\n",
              " '북한이 최근 유럽의회와 접촉해 ‘미국과 좋은 관계를 원한다’는 뜻을 밝혔다는 보도가 나왔다. 조 바이든 미국 행정부 출범을 앞둔 상황에서 북한이 도발보다는 대화에 방점을 둔 것인지 주목된다.',\n",
              " '월스트리트저널(WSJ)은 31일(현지시간) 소식통을 인용, 북한이 지난 11월 미 대선이 열리기 며칠 전 유럽의회 한반도관계대표단과 접촉해 온라인 면담을 요청해 성사됐다고 전했다.',\n",
              " '이번 면담은 주베를린 북한대사관이 제안했고 오스트리아 출신의 루카스 만들 유럽의회 한반도관계대표단 회장이 참석해 12월 초에 열렸다.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('paper_corpus.txt','w') as c:\n",
        "  for sent in train_data:\n",
        "    c.write(sent+'\\n')"
      ],
      "metadata": {
        "id": "zg9p6L1kvSvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NSMC"
      ],
      "metadata": {
        "id": "GC2mkEpqPcB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib"
      ],
      "metadata": {
        "id": "GAhQpLHSKpwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt','train.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGWRVyXKZjN",
        "outputId": "d7a99aa7-08a3-4f86-caaa-e0132862dfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('train.txt', <http.client.HTTPMessage at 0x79a87b7f9b40>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE7jFu1dpEiX",
        "outputId": "d9d948ef-5ba4-4b34-ae67-9262190808cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp=[]"
      ],
      "metadata": {
        "id": "_D-LYLxqLhUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.txt','r') as f:\n",
        "  f.readline()\n",
        "  for sent in f.readlines():\n",
        "    data = sent.split('\\t')\n",
        "    temp.append(data[1])"
      ],
      "metadata": {
        "id": "KRfZtCCTK8ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Crf3PXHL7x8",
        "outputId": "afff9864-c9cd-417c-a56b-5f95920e3197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Pytorch\\ NLP/GPT-2/data_in/gpt2_ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG3ehNgKKuDL",
        "outputId": "eccb70fe-aee4-4f40-b322-6515874ad5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in/gpt2_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('corpus.txt','w') as f:\n",
        "  for sent in temp:\n",
        "    f.write(f'\\n {sent}')"
      ],
      "metadata": {
        "id": "NaCJp9IsLLiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentencepiece"
      ],
      "metadata": {
        "id": "B2LJdAsrPbf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keep-steady.tistory.com/7"
      ],
      "metadata": {
        "id": "mBkBxNqCSM2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APOTeyaOOQYb",
        "outputId": "c713c095-b753-432d-af13-79f5b85effdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "ZayDEQQtPgz3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer from NSMC\n",
        "spm.SentencePieceTrainer.train(input='corpus.txt',model_prefix='kor_tokenizer',vocab_size=50257, model_type='bpe',max_sentence_length=9999999,pad_id=0,unk_id=1,bos_id=2,eos_id=3)"
      ],
      "metadata": {
        "id": "Z8G5cAI3PpbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcn-JiugwBi6",
        "outputId": "13e5596c-7b99-4d4c-964a-c5c15a8d076e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgpt2_ckpt\u001b[0m/  gpt_ckpt.zip         kor_tokenizer.vocab  train.json\n",
            "\u001b[01;34mgpt_ckpt\u001b[0m/   kor_tokenizer.model  paper_corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer from 신문 말뭉치 2022\n",
        "spm.SentencePieceTrainer.train(input='paper_corpus.txt',model_prefix='kor_tokenizer',vocab_size=50257, model_type='bpe',max_sentence_length=9999999,pad_id=0,unk_id=1,bos_id=2,eos_id=3)"
      ],
      "metadata": {
        "id": "C5w2qIh8u7Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "bCjotYQ6U329"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp=spm.SentencePieceProcessor()\n",
        "sp.Load('kor_tokenizer.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quvfnYo9rqdD",
        "outputId": "870c6c5b-45d5-4991-fff3-b9161bf32030"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.EncodeAsPieces('안녕하세요 저는 사람이에요.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdcRqlOLrYr-",
        "outputId": "866cedc0-8adc-4b1d-fefe-ff27dd5f641d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁안녕', '하세요', '▁저는', '▁사람이', '에', '요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode('안녕하세요 저는 사람이에요.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9ZNdSMwsE1T",
        "outputId": "75c4fceb-8c56-4d26-8c6e-3d261956bc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[41506, 17697, 10530, 3116, 48943, 49089, 48941]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode('그래')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eT_jqTlkfT0",
        "outputId": "8c0791eb-d787-4c11-aea0-257506f80bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[312]"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.bos_id()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvfou9ZbktX3",
        "outputId": "aa04198c-f4df-4ecc-9681-84ced74e9914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]"
      ],
      "metadata": {
        "id": "G4ZIARzH8IM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('0 :{}, 1 :{} , 2 :{}, 3 :{} '.format(vocab[0],vocab[1],vocab[2],vocab[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf1TMOXiioo2",
        "outputId": "2a126c08-2b57-4edc-f538-6dc3e0e5f56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :<pad>, 1 :<unk> , 2 :<s>, 3 :</s> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input for fine-tuning"
      ],
      "metadata": {
        "id": "EnUMRMLQwt3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtering (신문 데이터는 필터링 하지 않음)"
      ],
      "metadata": {
        "id": "VxowShiSyv5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Q5B8v-HhxFbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CMQA27SOx1jR",
        "outputId": "4b163680-8fa2-462c-90a3-8138f9b6b823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sent = [ ' '.join((re.sub('[^가-힣0-9 ]',' ',sent)).split()) for sent in temp]"
      ],
      "metadata": {
        "id": "Wop314V0w367"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sent[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GTlyEk2Hxnv-",
        "outputId": "10e7ba67-7193-4180-ddf3-ab68ce7021a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'볼때마다 눈물나서 죽겠다90년대의 향수자극 허진호는 감성절제멜로의 달인이다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### padding and truncation"
      ],
      "metadata": {
        "id": "OoKIFQIhy7wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NSMC"
      ],
      "metadata": {
        "id": "z7sf9ltayXob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "t9pzX7r4y6_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_len=[len(sp.encode(sent)) for sent in filtered_sent ]"
      ],
      "metadata": {
        "id": "XWNRSipb0CwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('3사분위 길이 :{}'.format(np.percentile(tokenized_len,99)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EYC0ibBzC_W",
        "outputId": "40fedf52-7f04-4e32-84aa-f40ca1ac2182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3사분위 길이 :47.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN=47"
      ],
      "metadata": {
        "id": "viy2W_q5mM0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "신문 말뭉치 2022"
      ],
      "metadata": {
        "id": "iFc-rv8iyY61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_len=[len(sp.encode(sent)) for sent in train_data ]"
      ],
      "metadata": {
        "id": "Upjqa1dRyXXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('3사분위 길이 :{}'.format(np.percentile(tokenized_len,75)))\n",
        "print('상위 99퍼 길이 :{}'.format(np.percentile(tokenized_len,99)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBao_9A7yd-j",
        "outputId": "982ac783-193c-4a90-bb90-2970f286aae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3사분위 길이 :52.0\n",
            "상위 99퍼 길이 :109.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어짜피 pad는 loss계산에서 제외할 것이기에 최대한 문장 길이를 길게 포함시키자\n",
        "MAX_LEN =100"
      ],
      "metadata": {
        "id": "6j_5OhBRy4Tz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ka_qkUUAp7Ei"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_output(input,max_len):\n",
        "  # input : (samples, sent_len)\n",
        "\n",
        "  train_input=[]\n",
        "  train_output=[]\n",
        "\n",
        "  # append <s>, </s>, <pad>\n",
        "  for sent in input:\n",
        "    if len(sp.encode(sent))<max_len:\n",
        "      pad_len = max_len-len(sp.encode(sent))-1\n",
        "      train_input.append([sp.bos_id()]+sp.encode(sent)+[sp.pad_id()]*pad_len)\n",
        "      train_output.append(sp.encode(sent)+[sp.eos_id()]+[sp.pad_id()]*pad_len)\n",
        "    else: # truncation\n",
        "      train_input.append([sp.bos_id()]+sp.encode(sent)[:max_len-1])\n",
        "      train_output.append(sp.encode(sent)[:max_len-1]+[sp.eos_id()])\n",
        "\n",
        "  return torch.LongTensor(train_input), torch.LongTensor(train_output) # (samples, max_len)"
      ],
      "metadata": {
        "id": "vd_P_Hp3mSYx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_output = preprocess_input_output(train_data,MAX_LEN)"
      ],
      "metadata": {
        "id": "mPjclEZApuzF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count =0\n",
        "for sent in train_input:\n",
        "  if sp.unk_id() in sent:\n",
        "    count+=1\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qewW2Jn60rlU",
        "outputId": "559812fb-975c-49f5-d9a8-6b3228ddfbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28520"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6pNb_feqHEO",
        "outputId": "0a1540b6-2dc5-41a5-9344-3d3aa9f8ada9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    2,    11, 49244, 48957,  1220, 27067,  3857,  1552,   257,  1271,\n",
              "        48988,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow_AgLq0qa63",
        "outputId": "a7d98157-85c5-43fc-9f8e-106e74e42e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   11, 49244, 48957,  1220, 27067,  3857,  1552,   257,  1271, 48988,\n",
              "            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader"
      ],
      "metadata": {
        "id": "UrALBW-GqeDu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.size()[0]"
      ],
      "metadata": {
        "id": "eNa1vjm2qjIl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_input,train_output)"
      ],
      "metadata": {
        "id": "DIP9A89F4G9H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = DataLoader(train_dataset,batch_size=32)"
      ],
      "metadata": {
        "id": "bBvvF-W65VcD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uujliuwn9D2w",
        "outputId": "e6543718-d614-4c32-983a-efdf68729260"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[    2,    11, 49244,  ...,     0,     0,     0],\n",
              "         [    2,  6393,   337,  ...,     0,     0,     0],\n",
              "         [    2,  7002, 49004,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [    2, 48838,  1199,  ...,     0,     0,     0],\n",
              "         [    2,   900,  4412,  ...,     0,     0,     0],\n",
              "         [    2, 24151,    59,  ...,     0,     0,     0]]),\n",
              " tensor([[   11, 49244, 48957,  ...,     0,     0,     0],\n",
              "         [ 6393,   337,  1220,  ...,     0,     0,     0],\n",
              "         [ 7002, 49004,  7690,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [48838,  1199,    54,  ...,     0,     0,     0],\n",
              "         [  900,  4412,   731,  ...,     0,     0,     0],\n",
              "         [24151,    59,   123,  ...,     0,     0,     0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### config & model_weight"
      ],
      "metadata": {
        "id": "SgiJQ2gTQn-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://huggingface.co/gpt2/resolve/main/config.json','config.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeqjKoUcQs2X",
        "outputId": "f4630f7e-722f-490c-98ca-fb15eaf06473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('config.json', <http.client.HTTPMessage at 0x7fdd07c1ada0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://huggingface.co/gpt2/resolve/main/model.safetensors','model.safetensors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXbpfCuvRWr2",
        "outputId": "fe5048c1-24e1-4700-cc03-420c0ad62500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model.safetensors', <http.client.HTTPMessage at 0x7fdd07c19ed0>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ~"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVQIbRLJ1g0i",
        "outputId": "6115b6f5-1f32-412c-c3d3-8f918f836e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO7mDJWg1hqy",
        "outputId": "3ca050be-39d5-4c4e-9d16-a245422711ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCichZVL1nli",
        "outputId": "7a2ffa8b-b112-42e6-bd83-86e9762eb712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Pytorch\\ NLP/GPT-2/data_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkJ_wyufzQc",
        "outputId": "43d38cfa-ae8f-4126-efd5-8cf9d8bb978d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic5DesXsf_zW",
        "outputId": "f52fe719-ff77-4046-9084-879a3df381f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=4ab74ff4e824669769cb28e4fd058e4a74225cd1b74e2f8689bd7d1bba7de6a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top k & Top p"
      ],
      "metadata": {
        "id": "0kjeBZ0bOhhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.multinomial"
      ],
      "metadata": {
        "id": "tuhXFcHsBYNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "SP56xQAnBlXK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "  # logits = (vocab_size, )\n",
        "\n",
        "  if top_k>0:\n",
        "    top_k=min(top_k,logits.size()[0])\n",
        "    indices_to_remove=logits<torch.topk(logits,top_k).values[-1] # smaller than the smallest value among the top_k values\n",
        "    logits[indices_to_remove]=filter_value\n",
        "\n",
        "  if top_p>0.0:\n",
        "    sorted_logits=torch.sort(logits,descending=True)\n",
        "    sorted_logits_index=torch.argsort(logits,descending=True)\n",
        "    prob_cumsum = torch.cumsum(nn.Softmax()(sorted_logits))\n",
        "\n",
        "    sorted_indices_to_remove=prob_cumsum>top_p\n",
        "    sorted_indices_to_remove=torch.cat([torch.LongTensor([False]),sorted_indices_to_remove[:-1]]) # prevent if first is True\n",
        "    indices_to_remove = sorted_logits_index[sorted_indices_to_remove]\n",
        "\n",
        "    logits[indices_to_remove]=filter_value\n",
        "\n",
        "  return logits   # logits = (vocab_size, )\n"
      ],
      "metadata": {
        "id": "Y3gxMcb8OxxL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy & Generate_sentence Function"
      ],
      "metadata": {
        "id": "-c0qYyVEqaoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sent(seed_word,model,max_len=100,greedy=False,top_k=0,top_p=0.0):\n",
        "  sent= seed_word\n",
        "  toked = sp.encode(sent)\n",
        "\n",
        "  for _ in range(max_len):\n",
        "    input_ids = torch.LongTensor([sp.bos_id()]+toked)[None,:] # input_ids = (1, cumulated_seq_len)\n",
        "    outputs = model(input_ids)[0,-1,:] # outputs : (vocab_size, )\n",
        "\n",
        "    if greedy:\n",
        "      gen = sp.id_to_piece(outputs.argmax().tolist()) # outputs.argmax().tolist() -> int\n",
        "    else:\n",
        "      output_logits = top_k_top_p_filtering(outputs,top_k,top_p) # logits = (vocab_size, )\n",
        "      gen = torch.multinomial(nn.Softmax(-1)(output_logits),num_samples=1,replacement=True).tolist()[0]\n",
        "      gen = sp.id_to_piece(gen)\n",
        "    if gen == '</s>':\n",
        "      break\n",
        "\n",
        "    sent+=gen.replace('▁',' ')\n",
        "    toked=sp.encode(sent)\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "7FD4jcD3eLBS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface Transformers"
      ],
      "metadata": {
        "id": "2tF56U0HqZSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MBQYWEjzU24",
        "outputId": "817875eb-fffd-4831-ae93-e75d78963c33"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgpt2_ckpt\u001b[0m/  gpt_ckpt.zip         kor_tokenizer.vocab  train.json\n",
            "\u001b[01;34mgpt_ckpt\u001b[0m/   kor_tokenizer.model  paper_corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au-SM5_crBt4",
        "outputId": "04a83755-1425-468f-fe05-d9d5a0564127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2/data_in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "uG_2d-qWo3Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bc21e5-6cf8-4ff2-f34a-d7fbbd7b0ca0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "oVRNhprurFxi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Pre-training"
      ],
      "metadata": {
        "id": "GPOoiqIA7Fd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Gen(nn.Module):\n",
        "  def __init__(self,dir_path):\n",
        "    super(GPT2Gen,self).__init__()\n",
        "    self.gpt2 = GPT2LMHeadModel.from_pretrained(dir_path,ignore_mismatched_sizes=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    outputs=self.gpt2(x)[0]\n",
        "    return torch.transpose(outputs,2,1) # (batch_size=1, vocab_size, max_len)"
      ],
      "metadata": {
        "id": "nz33hkvr74Tv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter\n",
        "MAX_LEN=100\n",
        "BATCH_SIZE=32\n",
        "EPOCHS=10\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "093hz24Q8fN3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Gen('./gpt2_ckpt').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=3e-5)"
      ],
      "metadata": {
        "id": "bcTT4Y-U7-nG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb=['gpt2.transformer.wte.weight','gpt2.transformer.wpe.weight']"
      ],
      "metadata": {
        "id": "lH6jgkK62zb_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in model.named_parameters():\n",
        "  if name not in emb:\n",
        "    param.requires_grad=False"
      ],
      "metadata": {
        "id": "4oZlRsMhcYOO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in model.named_parameters():\n",
        "  print(f'{name}, {param.requires_grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4IjYxsc-5h",
        "outputId": "588f46de-a665-4af6-b444-d185b00ae09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt2.transformer.wte.weight, True\n",
            "gpt2.transformer.wpe.weight, True\n",
            "gpt2.transformer.h.0.ln_1.weight, False\n",
            "gpt2.transformer.h.0.ln_1.bias, False\n",
            "gpt2.transformer.h.0.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.0.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.0.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.0.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.0.ln_2.weight, False\n",
            "gpt2.transformer.h.0.ln_2.bias, False\n",
            "gpt2.transformer.h.0.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.0.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.0.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.0.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.1.ln_1.weight, False\n",
            "gpt2.transformer.h.1.ln_1.bias, False\n",
            "gpt2.transformer.h.1.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.1.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.1.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.1.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.1.ln_2.weight, False\n",
            "gpt2.transformer.h.1.ln_2.bias, False\n",
            "gpt2.transformer.h.1.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.1.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.1.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.1.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.2.ln_1.weight, False\n",
            "gpt2.transformer.h.2.ln_1.bias, False\n",
            "gpt2.transformer.h.2.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.2.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.2.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.2.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.2.ln_2.weight, False\n",
            "gpt2.transformer.h.2.ln_2.bias, False\n",
            "gpt2.transformer.h.2.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.2.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.2.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.2.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.3.ln_1.weight, False\n",
            "gpt2.transformer.h.3.ln_1.bias, False\n",
            "gpt2.transformer.h.3.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.3.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.3.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.3.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.3.ln_2.weight, False\n",
            "gpt2.transformer.h.3.ln_2.bias, False\n",
            "gpt2.transformer.h.3.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.3.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.3.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.3.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.4.ln_1.weight, False\n",
            "gpt2.transformer.h.4.ln_1.bias, False\n",
            "gpt2.transformer.h.4.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.4.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.4.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.4.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.4.ln_2.weight, False\n",
            "gpt2.transformer.h.4.ln_2.bias, False\n",
            "gpt2.transformer.h.4.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.4.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.4.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.4.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.5.ln_1.weight, False\n",
            "gpt2.transformer.h.5.ln_1.bias, False\n",
            "gpt2.transformer.h.5.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.5.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.5.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.5.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.5.ln_2.weight, False\n",
            "gpt2.transformer.h.5.ln_2.bias, False\n",
            "gpt2.transformer.h.5.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.5.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.5.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.5.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.6.ln_1.weight, False\n",
            "gpt2.transformer.h.6.ln_1.bias, False\n",
            "gpt2.transformer.h.6.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.6.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.6.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.6.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.6.ln_2.weight, False\n",
            "gpt2.transformer.h.6.ln_2.bias, False\n",
            "gpt2.transformer.h.6.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.6.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.6.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.6.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.7.ln_1.weight, False\n",
            "gpt2.transformer.h.7.ln_1.bias, False\n",
            "gpt2.transformer.h.7.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.7.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.7.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.7.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.7.ln_2.weight, False\n",
            "gpt2.transformer.h.7.ln_2.bias, False\n",
            "gpt2.transformer.h.7.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.7.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.7.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.7.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.8.ln_1.weight, False\n",
            "gpt2.transformer.h.8.ln_1.bias, False\n",
            "gpt2.transformer.h.8.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.8.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.8.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.8.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.8.ln_2.weight, False\n",
            "gpt2.transformer.h.8.ln_2.bias, False\n",
            "gpt2.transformer.h.8.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.8.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.8.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.8.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.9.ln_1.weight, False\n",
            "gpt2.transformer.h.9.ln_1.bias, False\n",
            "gpt2.transformer.h.9.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.9.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.9.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.9.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.9.ln_2.weight, False\n",
            "gpt2.transformer.h.9.ln_2.bias, False\n",
            "gpt2.transformer.h.9.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.9.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.9.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.9.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.10.ln_1.weight, False\n",
            "gpt2.transformer.h.10.ln_1.bias, False\n",
            "gpt2.transformer.h.10.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.10.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.10.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.10.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.10.ln_2.weight, False\n",
            "gpt2.transformer.h.10.ln_2.bias, False\n",
            "gpt2.transformer.h.10.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.10.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.10.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.10.mlp.c_proj.bias, False\n",
            "gpt2.transformer.h.11.ln_1.weight, False\n",
            "gpt2.transformer.h.11.ln_1.bias, False\n",
            "gpt2.transformer.h.11.attn.c_attn.weight, False\n",
            "gpt2.transformer.h.11.attn.c_attn.bias, False\n",
            "gpt2.transformer.h.11.attn.c_proj.weight, False\n",
            "gpt2.transformer.h.11.attn.c_proj.bias, False\n",
            "gpt2.transformer.h.11.ln_2.weight, False\n",
            "gpt2.transformer.h.11.ln_2.bias, False\n",
            "gpt2.transformer.h.11.mlp.c_fc.weight, False\n",
            "gpt2.transformer.h.11.mlp.c_fc.bias, False\n",
            "gpt2.transformer.h.11.mlp.c_proj.weight, False\n",
            "gpt2.transformer.h.11.mlp.c_proj.bias, False\n",
            "gpt2.transformer.ln_f.weight, False\n",
            "gpt2.transformer.ln_f.bias, False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8dwuo8L69JBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.Tensor([[1,1,1,0,0],[1,1,0,0,0]])\n",
        "mask=~(a==0)\n",
        "\n",
        "b=torch.Tensor([[3,2,5,0,0],[1,5,3,1,0]])\n",
        "b*mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucore8EoA3D_",
        "outputId": "d21d24eb-716d-4e5a-d49d-1a7d64f1f3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 2., 5., 0., 0.],\n",
              "        [1., 5., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(outputs,y,criterion):\n",
        "  loss = criterion(outputs,y) # (batch_size, max_len)\n",
        "  mask = ~(y==0)\n",
        "\n",
        "  return (loss*mask).sum()"
      ],
      "metadata": {
        "id": "SwvBnFth_7Ml"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer,criterion,train_iter):\n",
        "  model.train()\n",
        "  losses=[]\n",
        "  count=0\n",
        "  total_loss = 0\n",
        "  for batch in tqdm(train_iter):\n",
        "    if count==5000:\n",
        "      break\n",
        "    count+=1\n",
        "    x=batch[0].to(device)\n",
        "    y=batch[1].to(device)\n",
        "\n",
        "    outputs = model(x) # outputs : (batch_size, vocab_size, max_len)\n",
        "    loss = compute_loss(outputs,y,criterion) # (batch_size, max_len )\n",
        "    total_loss+=loss\n",
        "    losses.append(loss)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return total_loss/len(train_iter), losses"
      ],
      "metadata": {
        "id": "U3W9CSsX75MN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "88Ea4Y_vEPqw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jqe1kp03R_Q",
        "outputId": "b22f3383-ccca-4cd9-9ce5-b601cda9103a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = None\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  avg_loss, losses=train(model,optimizer,criterion,train_iter)\n",
        "  print('avg_loss : {}'.format(avg_loss))\n",
        "\n",
        "  if not best_loss or avg_loss<best_loss:\n",
        "    os.makedirs('./data_out/best_model',exist_ok=True)\n",
        "    torch.save(model.state_dict(),'./data_out/best_model/best_weight.pt')\n",
        "    best_loss=avg_loss"
      ],
      "metadata": {
        "id": "zfnyJcctCc-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "4de0763f-334e-49b1-8cd5-315de922291c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 5000/22064 [1:06:20<3:46:25,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_loss : 2704.3212890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-55bb716d4f53>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_out/best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./data_out/best_model/best_weight.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './data_out/best_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  if not best_loss or avg_loss<best_loss:\n",
        "    os.makedirs('./data_out/best_model',exist_ok=True)\n",
        "    torch.save(model.state_dict(),'./data_out/best_model/best_weight.pt')\n",
        "    best_loss=avg_loss"
      ],
      "metadata": {
        "id": "HXO495IHDPGq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "S-g0_tEs_iZQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses=[loss.tolist() for loss in losses]"
      ],
      "metadata": {
        "id": "gmuwdEXzDeKU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Vzr96KEoBt",
        "outputId": "87ac7a77-9374-49d4-a637-2e98ad679a91"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata_in\u001b[0m/  \u001b[01;34mdata_out\u001b[0m/  GPT2_GEN.ipynb  losses.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('./losses',np.array(losses))"
      ],
      "metadata": {
        "id": "E8-vaU-b_xxi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "s9FsViDKErbV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVeuPfFNM2Ta",
        "outputId": "a07f4985-8dd7-45c0-c534-0d744ea07e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['gpt2.transformer.wte.weight', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.h.6.ln_1.weight', 'gpt2.transformer.h.6.ln_1.bias', 'gpt2.transformer.h.6.attn.c_attn.weight', 'gpt2.transformer.h.6.attn.c_attn.bias', 'gpt2.transformer.h.6.attn.c_proj.weight', 'gpt2.transformer.h.6.attn.c_proj.bias', 'gpt2.transformer.h.6.ln_2.weight', 'gpt2.transformer.h.6.ln_2.bias', 'gpt2.transformer.h.6.mlp.c_fc.weight', 'gpt2.transformer.h.6.mlp.c_fc.bias', 'gpt2.transformer.h.6.mlp.c_proj.weight', 'gpt2.transformer.h.6.mlp.c_proj.bias', 'gpt2.transformer.h.7.ln_1.weight', 'gpt2.transformer.h.7.ln_1.bias', 'gpt2.transformer.h.7.attn.c_attn.weight', 'gpt2.transformer.h.7.attn.c_attn.bias', 'gpt2.transformer.h.7.attn.c_proj.weight', 'gpt2.transformer.h.7.attn.c_proj.bias', 'gpt2.transformer.h.7.ln_2.weight', 'gpt2.transformer.h.7.ln_2.bias', 'gpt2.transformer.h.7.mlp.c_fc.weight', 'gpt2.transformer.h.7.mlp.c_fc.bias', 'gpt2.transformer.h.7.mlp.c_proj.weight', 'gpt2.transformer.h.7.mlp.c_proj.bias', 'gpt2.transformer.h.8.ln_1.weight', 'gpt2.transformer.h.8.ln_1.bias', 'gpt2.transformer.h.8.attn.c_attn.weight', 'gpt2.transformer.h.8.attn.c_attn.bias', 'gpt2.transformer.h.8.attn.c_proj.weight', 'gpt2.transformer.h.8.attn.c_proj.bias', 'gpt2.transformer.h.8.ln_2.weight', 'gpt2.transformer.h.8.ln_2.bias', 'gpt2.transformer.h.8.mlp.c_fc.weight', 'gpt2.transformer.h.8.mlp.c_fc.bias', 'gpt2.transformer.h.8.mlp.c_proj.weight', 'gpt2.transformer.h.8.mlp.c_proj.bias', 'gpt2.transformer.h.9.ln_1.weight', 'gpt2.transformer.h.9.ln_1.bias', 'gpt2.transformer.h.9.attn.c_attn.weight', 'gpt2.transformer.h.9.attn.c_attn.bias', 'gpt2.transformer.h.9.attn.c_proj.weight', 'gpt2.transformer.h.9.attn.c_proj.bias', 'gpt2.transformer.h.9.ln_2.weight', 'gpt2.transformer.h.9.ln_2.bias', 'gpt2.transformer.h.9.mlp.c_fc.weight', 'gpt2.transformer.h.9.mlp.c_fc.bias', 'gpt2.transformer.h.9.mlp.c_proj.weight', 'gpt2.transformer.h.9.mlp.c_proj.bias', 'gpt2.transformer.h.10.ln_1.weight', 'gpt2.transformer.h.10.ln_1.bias', 'gpt2.transformer.h.10.attn.c_attn.weight', 'gpt2.transformer.h.10.attn.c_attn.bias', 'gpt2.transformer.h.10.attn.c_proj.weight', 'gpt2.transformer.h.10.attn.c_proj.bias', 'gpt2.transformer.h.10.ln_2.weight', 'gpt2.transformer.h.10.ln_2.bias', 'gpt2.transformer.h.10.mlp.c_fc.weight', 'gpt2.transformer.h.10.mlp.c_fc.bias', 'gpt2.transformer.h.10.mlp.c_proj.weight', 'gpt2.transformer.h.10.mlp.c_proj.bias', 'gpt2.transformer.h.11.ln_1.weight', 'gpt2.transformer.h.11.ln_1.bias', 'gpt2.transformer.h.11.attn.c_attn.weight', 'gpt2.transformer.h.11.attn.c_attn.bias', 'gpt2.transformer.h.11.attn.c_proj.weight', 'gpt2.transformer.h.11.attn.c_proj.bias', 'gpt2.transformer.h.11.ln_2.weight', 'gpt2.transformer.h.11.ln_2.bias', 'gpt2.transformer.h.11.mlp.c_fc.weight', 'gpt2.transformer.h.11.mlp.c_fc.bias', 'gpt2.transformer.h.11.mlp.c_proj.weight', 'gpt2.transformer.h.11.mlp.c_proj.bias', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.ln_f.bias', 'gpt2.lm_head.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGwm5vhITZN-",
        "outputId": "593107ff-a40e-423f-dfd2-b41a2253d11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Gen(\n",
              "  (gpt2): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generative Model"
      ],
      "metadata": {
        "id": "4JZU6PS-rdS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK5seQ5Qr8eI",
        "outputId": "9d961767-e3c4-4b74-fac0-c83a3cd0c832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch NLP/GPT-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Gen(nn.Module):\n",
        "  def __init__(self,dir_path):\n",
        "    super(GPT2Gen,self).__init__()\n",
        "    self.gpt2 = GPT2LMHeadModel.from_pretrained(dir_path,ignore_mismatched_sizes=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.gpt2(x)[0] # (batch_size=1, cumulated_seq_len, vocab_size =500257)"
      ],
      "metadata": {
        "id": "O-FSehzxrOCz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model = GPT2Gen('./data_in/gpt2_ckpt')"
      ],
      "metadata": {
        "id": "5H6yvm_Jsdw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate_sentences"
      ],
      "metadata": {
        "id": "slaZ1EEntEWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('data_out/best_model/best_weight.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp5ytgvIlqnr",
        "outputId": "75151fc4-fccf-44a8-dcc8-024fbb7f5093"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfCwuzOsmrD0",
        "outputId": "65de0739-8975-4198-b10e-8e4346980584"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Gen(\n",
              "  (gpt2): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent('그래',model,max_len=10,top_k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TK_KLic3tB6P",
        "outputId": "ce47cb58-9347-48ea-e929-b9e2db7eaea2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'그래<pad><pad><pad> 지<pad> 2<pad> “<pad> 있다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "mxImBf-LtQwz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}